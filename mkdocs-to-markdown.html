<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MkDocs to Markdown</title>
  <link rel="stylesheet" href="style.css">
  <script type="module" src="js/common.js"></script>
  <style>
* { 
  box-sizing: border-box;
}

  textarea {
  height: 400px;
  font-family: monospace;
}

.status {
  margin: 10px 0;
  padding: 10px;
  border-radius: 4px;
}

.progress {
  margin: 10px 0;
  padding: 10px;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.progress-bar {
  height: 20px;
  background: #eee;
  border-radius: 4px;
  overflow: hidden;
}

.progress-fill {
  height: 100%;
  background: #0066cc;
  transition: width 0.3s ease;
}

.stats {
  display: flex;
  justify-content: space-between;
  margin-top: 5px;
  font-size: 14px;
  color: #666;
}
  </style>
</head>
<body>
  <div class="container">
  <h1>MkDocs to Markdown</h1>
  <p>Convert a MkDocs site to Markdown.</p>
  
  <div>
    <input type="text" id="url" placeholder="Enter site domain (e.g., https://www.mkdocs.org/)" value="https://www.mkdocs.org/">
    <button id="fetch">Crawl All Pages</button>
  </div>

  <div class="progress" style="display: none">
    <div class="progress-bar">
      <div class="progress-fill" style="width: 0%"></div>
    </div>
    <div class="stats">
      <span>Pages processed: <span id="processed">0</span></span>
      <span>Total pages: <span id="total">0</span></span>
    </div>
  </div>

  <div id="status"></div>
  
  <textarea id="output" readonly></textarea>
  
  <div>
    <button id="download" disabled>Download Markdown</button>
    <button id="copy" disabled>Copy to Clipboard</button>
  </div>
</div>
  <script type="module">
// Config
const CONFIG = {
  PROXY_BASE: 'https://lab-proxy.vercel.app/api/proxy?url=',
  SITEMAP_PATHS: [
    '/sitemap.xml',
    '/sitemap_index.xml',
    '/sitemap/',
    '/api/sitemap',
    '/docs/sitemap.xml'
  ],
  // Maximum number of pages to crawl if no sitemap found
  MAX_CRAWL_PAGES: 100
}

// State
const state = {
  baseUrl: '',
  sitePathPrefix: '', // Store the path prefix (e.g., '/mkdocs')
  processingQueue: []
}

// Show status message
function showStatus(message, isError = false) {
  const status = document.getElementById('status')
  status.textContent = message
  status.className = 'status ' + (isError ? 'error' : 'success')
}

// Enable/disable buttons
function setButtonsEnabled(enabled) {
  document.getElementById('fetch').disabled = !enabled
  document.getElementById('download').disabled = !enabled
  document.getElementById('copy').disabled = !enabled
}

// Update progress display
function updateProgress(processed, total) {
  document.querySelector('.progress').style.display = 'block'
  document.getElementById('processed').textContent = processed
  document.getElementById('total').textContent = total
  
  const progress = (processed / total) * 100
  document.querySelector('.progress-fill').style.width = `${progress}%`
}

// Helper function for fetching with fallback
async function fetchWithFallback(url) {
  try {
    // Try direct request first
    const directResponse = await fetch(url)
    if (directResponse.ok) {
      return directResponse
    }
  } catch (error) {
    console.log('Direct request failed, trying proxy:', error)
  }

  // If direct request fails, try proxy
  const proxiedUrl = CONFIG.PROXY_BASE + encodeURIComponent(url)
  const proxyResponse = await fetch(proxiedUrl)
  
  if (!proxyResponse.ok) {
    throw new Error(`Failed to fetch: ${proxyResponse.status}`)
  }
  
  return proxyResponse
}

// Fetch and parse sitemap
// Fetch and parse sitemap with fallback to crawling
async function fetchSitemap(siteUrl) {
  // Remove trailing slashes and normalize URL
  const baseUrl = siteUrl.replace(/\/+$/, '')
  
  // Try each possible sitemap location
  for (const path of CONFIG.SITEMAP_PATHS) {
    const sitemapUrl = `${baseUrl}${path}`
    console.log('Trying sitemap at:', sitemapUrl)
    
    try {
      const response = await fetchWithFallback(sitemapUrl)
      const text = await response.text()
      
      // Check if it's actually XML and not HTML
      if (!text.trim().startsWith('<?xml')) {
        console.log('Response is not XML, skipping:', sitemapUrl)
        continue
      }
      
      const parser = new DOMParser()
      const xml = parser.parseFromString(text, 'text/xml')
      
      // Check for parse errors
      const parseError = xml.querySelector('parsererror')
      if (parseError) {
        console.log('XML parse error, skipping:', sitemapUrl)
        continue
      }
      
      // Extract URLs from sitemap
      const urls = new Set()
      
      // Handle both namespaced and non-namespaced XML
      function getElements(tagName) {
        const elements = [
          ...xml.getElementsByTagNameNS('http://www.sitemaps.org/schemas/sitemap/0.9', tagName),
          ...xml.getElementsByTagName(tagName)
        ]
        return elements
      }
      
      // Check for sitemap index
      const sitemaps = getElements('sitemap')
      if (sitemaps.length > 0) {
        for (const sitemap of sitemaps) {
          const loc = sitemap.querySelector('loc')
          if (loc) {
            try {
              const subSitemapUrl = loc.textContent.trim()
              console.log('Fetching sub-sitemap:', subSitemapUrl)
              const subUrls = await fetchSitemap(subSitemapUrl)
              subUrls.forEach(url => urls.add(url))
            } catch (e) {
              console.warn('Error fetching sub-sitemap:', e)
            }
          }
        }
      }
      
      // Get URLs from current sitemap
      const urlElements = getElements('url')
      for (const url of urlElements) {
        const loc = url.querySelector('loc')
        if (loc) {
          const urlText = loc.textContent.trim()
          try {
            const urlObj = new URL(urlText)
            if (urlObj.hostname.includes(state.baseUrl)) {
              urls.add(urlText)
            }
          } catch (e) {
            console.warn('Invalid URL in sitemap:', urlText)
          }
        }
      }
      
      if (urls.size > 0) {
        return Array.from(urls)
      }
      
    } catch (error) {
      console.warn(`Error checking sitemap at ${sitemapUrl}:`, error)
    }
  }
  
  // If no sitemap found, fall back to crawling
  console.log('No sitemap found, falling back to crawling')
  return crawlSite(baseUrl)
}

// Fallback crawler function
async function crawlSite(baseUrl) {
  const urls = new Set()
  const queue = [baseUrl]
  const seen = new Set()
  
  while (queue.length > 0 && urls.size < CONFIG.MAX_CRAWL_PAGES) {
    const url = queue.shift()
    if (seen.has(url)) continue
    seen.add(url)
    
    try {
      console.log('Crawling:', url)
      const response = await fetchWithFallback(url)
      
      if (!response.ok) continue
      
      const text = await response.text()
      const parser = new DOMParser()
      const doc = parser.parseFromString(text, 'text/html')
      
      // Add current URL if it's a content page
      if (url !== baseUrl) {
        urls.add(url)
      }
      
      // Find all links
      const links = doc.querySelectorAll('a[href]')
      for (const link of links) {
        try {
          const href = new URL(link.href, url).toString()
          const urlObj = new URL(href)
          
          // Only include URLs from same domain and not already seen
          if (urlObj.hostname.includes(state.baseUrl) && 
              !seen.has(href) && 
              !href.includes('#') && // Skip anchors
              !/\.(jpg|jpeg|png|gif|css|js)$/i.test(href)) { // Skip assets
            queue.push(href)
          }
        } catch (e) {
          console.warn('Invalid URL:', link.href)
        }
      }
    } catch (error) {
      console.warn(`Error crawling ${url}:`, error)
    }
  }
  
  return Array.from(urls)
}

// Fetch and process content
async function fetchContent(url) {
  // Rewrite URL if needed
  let fetchUrl = url
  try {
    const urlObj = new URL(url)
    if (urlObj.hostname.startsWith('docs.')) {
      // Replace docs.domain.com/path with domain.com/mkdocs/path
      const path = urlObj.pathname
      fetchUrl = `https://${state.baseUrl}/${state.sitePathPrefix}${path}`
    }
  } catch (e) {
    console.warn('Invalid URL for rewriting:', url)
  }
  
  console.log('Original URL:', url)
  console.log('Fetching from:', fetchUrl)
  
  const response = await fetchWithFallback(fetchUrl)
  const html = await response.text()
  return html
}

// Convert HTML to Markdown
function htmlToMarkdown(html, url) {
  const parser = new DOMParser()
  const doc = parser.parseFromString(html, 'text/html')
  
  // Find main content - try multiple possible MkDocs selectors
  const contentSelectors = [
    '.md-content__inner',          // Standard MkDocs
    'main',                        // Generic main content
    '.content',                    // Generic content
    'article',                     // Generic article
    '.col-md-9',                  // MkDocs older versions
    '[role="main"]',              // MkDocs role attribute
    '.md-main__inner',            // MkDocs material theme
    '.md-content',                // MkDocs content wrapper
    '#main-content'               // MkDocs common ID
  ]
  
  let mainContent = null
  for (const selector of contentSelectors) {
    mainContent = doc.querySelector(selector)
    if (mainContent) {
      console.log('Found content using selector:', selector)
      break
    }
  }

  if (!mainContent) {
    console.warn('No main content found with standard selectors, falling back to body')
    mainContent = doc.body
  }

  let markdown = `# ${url}\n\n`

  // Process content recursively
  function processNode(node) {
    if (node.nodeType === 3) { // Text node
      return node.textContent.trim()
    }
    
    if (node.nodeType !== 1) { // Not an element
      return ''
    }

    const tag = node.tagName.toLowerCase()
    let content = ''

    switch (tag) {
      case 'h1':
      case 'h2':
      case 'h3':
      case 'h4':
      case 'h5':
      case 'h6':
        const level = tag.charAt(1)
        content = `${'#'.repeat(level)} ${node.textContent.trim()}\n\n`
        break

      case 'p':
        content = `${node.textContent.trim()}\n\n`
        break

      case 'pre':
        const code = node.querySelector('code')
        const language = code?.className.replace('language-', '') || ''
        content = `\`\`\`${language}\n${node.textContent.trim()}\n\`\`\`\n\n`
        break

      case 'ul':
      case 'ol':
        node.querySelectorAll('li').forEach(li => {
          content += `* ${li.textContent.trim()}\n`
        })
        content += '\n'
        break

      case 'blockquote':
        content = `> ${node.textContent.trim()}\n\n`
        break

      case 'table':
        // Basic table support
        const rows = node.querySelectorAll('tr')
        rows.forEach((row, i) => {
          const cells = Array.from(row.querySelectorAll('td, th'))
            .map(cell => cell.textContent.trim())
            .join(' | ')
          content += `| ${cells} |\n`
          
          // Add separator after header
          if (i === 0) {
            content += `|${cells.split('|').map(() => ' --- |').join('')}\n`
          }
        })
        content += '\n'
        break

      default:
        // Recursively process child nodes
        Array.from(node.childNodes).forEach(child => {
          content += processNode(child)
        })
    }

    return content
  }

  markdown += processNode(mainContent)
  markdown += '\n---\n\n' // Page separator
  return markdown.trim()
}

// Download markdown file
function downloadMarkdown(content, filename = 'content.md') {
  const blob = new Blob([content], { type: 'text/markdown' })
  const url = URL.createObjectURL(blob)
  const a = document.createElement('a')
  a.href = url
  a.download = filename
  document.body.appendChild(a)
  a.click()
  document.body.removeChild(a)
  URL.revokeObjectURL(url)
}

// Initialize event listeners
document.getElementById('fetch').addEventListener('click', async () => {
  const urlInput = document.getElementById('url')
  const outputArea = document.getElementById('output')
  
  // Extract domain and path prefix for filtering
  try {
    const urlObj = new URL(urlInput.value.trim())
    const mainDomain = urlObj.hostname.split('.').slice(-2).join('.') 
    state.baseUrl = mainDomain
    state.sitePathPrefix = urlObj.pathname.split('/')[1] // Get the first path segment
  } catch (e) {
    showStatus('Invalid URL provided', true)
    return
  }
  
  // Reset state
  outputArea.value = ''
  setButtonsEnabled(false)
  
  try {
    showStatus('Searching for sitemap...')
    const urls = await fetchSitemap(urlInput.value)
    
    if (urls.length === 0) {
      showStatus('No pages found to process', true)
      setButtonsEnabled(true)
      return
    }
    
    showStatus(`Found ${urls.length} pages. Starting crawl...`)
    let combinedMarkdown = ''
    let processed = 0
    
    for (const url of urls) {
      try {
        const html = await fetchContent(url)
        const markdown = htmlToMarkdown(html, url)
        combinedMarkdown += markdown + '\n\n'
        
        processed++
        updateProgress(processed, urls.length)
        outputArea.value = combinedMarkdown
      } catch (error) {
        console.warn(`Error processing ${url}:`, error)
        combinedMarkdown += `# Error: ${url}\n\n${error.message}\n\n---\n\n`
      }
    }
    
    showStatus(`Crawl complete! Processed ${processed} pages.`)
    setButtonsEnabled(true)
  } catch (error) {
    console.error('Crawler error:', error)
    showStatus(error.message, true)
    setButtonsEnabled(true)
  }
})

document.getElementById('download').addEventListener('click', () => {
  const content = document.getElementById('output').value
  if (content) {
    const urlInput = document.getElementById('url')
    const siteName = new URL(urlInput.value).hostname
    downloadMarkdown(content, `${siteName}-docs.md`)
  }
})

document.getElementById('copy').addEventListener('click', async () => {
  const content = document.getElementById('output').value
  if (content) {
    await navigator.clipboard.writeText(content)
    showStatus('Content copied to clipboard!')
  }
})
  </script>
</body>
</html>